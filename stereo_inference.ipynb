{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.stereo_inference import StereoInferenceOnnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images\n",
    "left_image = cv2.imread(\"data/samples/seat_l_n.png\", cv2.IMREAD_COLOR)\n",
    "right_image = cv2.imread(\"data/samples/seat_r_n.png\", cv2.IMREAD_COLOR)\n",
    "# Convert from BGR to RGB\n",
    "left_image_rgb = cv2.cvtColor(left_image, cv2.COLOR_BGR2RGB)\n",
    "right_image_rgb = cv2.cvtColor(right_image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the stereo pair\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(left_image_rgb)\n",
    "plt.title('Left Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(right_image_rgb)\n",
    "plt.title('Right Image')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ONNX model\n",
    "model_path = \"models/fs_768_1024.onnx\"\n",
    "inference_engine = StereoInferenceOnnx(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess images for the model\n",
    "def preprocess_images(left_img, right_img, target_height=None, target_width=None):\n",
    "    \"\"\"\n",
    "    Preprocess images for ONNX inference:\n",
    "    1. Resize if target dimensions are provided\n",
    "    2. Convert to RGB if needed\n",
    "    3. Normalize to float32 [0-255]\n",
    "    4. Transpose from HWC to CHW format\n",
    "    5. Add batch dimension\n",
    "    \"\"\"\n",
    "    # Resize if dimensions are provided\n",
    "    if target_height is not None and target_width is not None:\n",
    "        left_img = cv2.resize(left_img, (target_width, target_height))\n",
    "        right_img = cv2.resize(right_img, (target_width, target_height))\n",
    "    \n",
    "    # Get current dimensions\n",
    "    height, width = left_img.shape[:2]\n",
    "    \n",
    "    # Convert to RGB if in BGR format\n",
    "    if left_img.shape[2] == 3:  # If color image\n",
    "        left_img = cv2.cvtColor(left_img, cv2.COLOR_BGR2RGB)\n",
    "        right_img = cv2.cvtColor(right_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to float32 [0-255]\n",
    "    left_img = left_img.astype(np.float32)\n",
    "    right_img = right_img.astype(np.float32)\n",
    "    \n",
    "    # Transpose from HWC to CHW format\n",
    "    left_img = left_img.transpose(2, 0, 1)  # (C, H, W)\n",
    "    right_img = right_img.transpose(2, 0, 1)  # (C, H, W)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    left_img = np.expand_dims(left_img, axis=0)  # (1, C, H, W)\n",
    "    right_img = np.expand_dims(right_img, axis=0)  # (1, C, H, W)\n",
    "    \n",
    "    return left_img, right_img, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_height = 1024\n",
    "target_width = 768\n",
    "\n",
    "left_tensor, right_tensor, orig_height, orig_width = preprocess_images(\n",
    "    left_image, right_image, target_height, target_width\n",
    ")\n",
    "\n",
    "print(f\"Input tensor shapes: {left_tensor.shape}, {right_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52901829",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity = inference_engine(left_tensor, right_tensor)\n",
    "print(f\"Disparity output shape: {disparity.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32990a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_disparity(disparity_map, min_disp=None, max_disp=None):\n",
    "    \"\"\"\n",
    "    Visualize disparity map with optional min/max normalization.\n",
    "    Returns a colorized disparity map.\n",
    "    \"\"\"\n",
    "    # Remove batch dimension if present\n",
    "    if disparity_map.ndim == 4:\n",
    "        disparity_map = disparity_map[0]\n",
    "    if disparity_map.ndim == 3:\n",
    "        disparity_map = disparity_map[0]  # Keep only the first channel\n",
    "    \n",
    "    # Handle NaN and Inf values\n",
    "    disparity_map = np.nan_to_num(disparity_map, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Normalize disparity to 0-1 range for visualization\n",
    "    if min_disp is None:\n",
    "        min_disp = np.min(disparity_map)\n",
    "    if max_disp is None:\n",
    "        max_disp = np.max(disparity_map)\n",
    "    \n",
    "    # Clip values and normalize\n",
    "    normalized_disparity = np.clip(disparity_map, min_disp, max_disp)\n",
    "    normalized_disparity = (normalized_disparity - min_disp) / (max_disp - min_disp)\n",
    "    \n",
    "    # Apply colormap\n",
    "    colored_disparity = cv2.applyColorMap((normalized_disparity * 255).astype(np.uint8), \n",
    "                                          cv2.COLORMAP_TURBO)\n",
    "    colored_disparity = cv2.cvtColor(colored_disparity, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return colored_disparity, normalized_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c846e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the disparity map\n",
    "colored_disparity, normalized_disparity = visualize_disparity(disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bacc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(left_image_rgb)\n",
    "plt.title('Original Left Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(colored_disparity)\n",
    "plt.title('Disparity Map')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Open3D for point cloud processing\n",
    "import open3d as o3d\n",
    "\n",
    "# Helper functions from the original implementation\n",
    "def depth2xyzmap(depth, K):\n",
    "    \"\"\"Convert depth map to 3D point cloud in camera coordinates.\"\"\"\n",
    "    h, w = depth.shape\n",
    "    y, x = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\n",
    "    \n",
    "    # Convert image coordinates to normalized device coordinates\n",
    "    x = (x - K[0, 2]) / K[0, 0]\n",
    "    y = (y - K[1, 2]) / K[1, 1]\n",
    "    \n",
    "    # Create homogeneous coordinates\n",
    "    xyz = np.stack([x, y, np.ones_like(depth)], axis=-1)\n",
    "    \n",
    "    # Scale by depth\n",
    "    xyz = xyz * depth[..., np.newaxis]\n",
    "    \n",
    "    return xyz\n",
    "\n",
    "def toOpen3dCloud(xyz, rgb=None):\n",
    "    \"\"\"Convert point cloud data to Open3D format.\"\"\"\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "    \n",
    "    if rgb is not None:\n",
    "        # Normalize RGB to [0, 1]\n",
    "        if rgb.max() > 1:\n",
    "            rgb = rgb / 255.0\n",
    "        pcd.colors = o3d.utility.Vector3dVector(rgb)\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "# %%\n",
    "# Generate and visualize point cloud\n",
    "def generate_point_cloud(disparity_map, intrinsic_file, scale=1.0, z_far=10.0, \n",
    "                        denoise_cloud=True, denoise_nb_points=30, denoise_radius=0.03,\n",
    "                        output_dir=\"output/\"):\n",
    "    \"\"\"\n",
    "    Generate point cloud from disparity map using camera intrinsics.\n",
    "    \n",
    "    Args:\n",
    "        disparity_map: Disparity map as numpy array\n",
    "        intrinsic_file: Path to file containing camera intrinsics\n",
    "        scale: Scale factor (if images were resized)\n",
    "        z_far: Maximum depth to clip in point cloud\n",
    "        denoise_cloud: Whether to denoise the point cloud\n",
    "        denoise_nb_points: Number of points to consider for radius outlier removal\n",
    "        denoise_radius: Radius to use for outlier removal\n",
    "        output_dir: Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        Point cloud object\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Remove batch dimension if present\n",
    "    if disparity_map.ndim == 4:\n",
    "        disparity_map = disparity_map[0, 0]\n",
    "    elif disparity_map.ndim == 3:\n",
    "        disparity_map = disparity_map[0]\n",
    "    \n",
    "    # Load camera intrinsics\n",
    "    with open(intrinsic_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        K = np.array(list(map(float, lines[0].rstrip().split()))).astype(np.float32).reshape(3, 3)\n",
    "        baseline = float(lines[1])\n",
    "    \n",
    "    # Scale intrinsics if images were resized\n",
    "    K[:2] *= scale\n",
    "    \n",
    "    # Calculate depth from disparity\n",
    "    # Avoid division by zero\n",
    "    disparity_map = np.maximum(disparity_map, 0.1)\n",
    "    depth = K[0, 0] * baseline / disparity_map\n",
    "    \n",
    "    # Save depth map\n",
    "    np.save(f'{output_dir}/depth_meter.npy', depth)\n",
    "    \n",
    "    # Use original RGB image for coloring\n",
    "    img_rgb = left_image_rgb\n",
    "    \n",
    "    # Convert depth to 3D points\n",
    "    xyz_map = depth2xyzmap(depth, K)\n",
    "    \n",
    "    # Create point cloud\n",
    "    pcd = toOpen3dCloud(xyz_map.reshape(-1, 3), img_rgb.reshape(-1, 3))\n",
    "    \n",
    "    # Filter points by depth range\n",
    "    keep_mask = (np.asarray(pcd.points)[:, 2] > 0) & (np.asarray(pcd.points)[:, 2] <= z_far)\n",
    "    keep_ids = np.arange(len(np.asarray(pcd.points)))[keep_mask]\n",
    "    pcd = pcd.select_by_index(keep_ids)\n",
    "    \n",
    "    # Save point cloud\n",
    "    o3d.io.write_point_cloud(f'{output_dir}/cloud.ply', pcd)\n",
    "    print(f\"Point cloud saved to {output_dir}/cloud.ply\")\n",
    "    \n",
    "    # Denoise point cloud if requested\n",
    "    if denoise_cloud:\n",
    "        print(\"Denoising point cloud...\")\n",
    "        cl, ind = pcd.remove_radius_outlier(nb_points=denoise_nb_points, radius=denoise_radius)\n",
    "        inlier_cloud = pcd.select_by_index(ind)\n",
    "        o3d.io.write_point_cloud(f'{output_dir}/cloud_denoise.ply', inlier_cloud)\n",
    "        print(f\"Denoised point cloud saved to {output_dir}/cloud_denoise.ply\")\n",
    "        pcd = inlier_cloud\n",
    "    \n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48288fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_file = \"camera_configs/K.txt\"  # Path to your intrinsic file\n",
    "scale = 1/4  # Set to the scale factor if images were resized\n",
    "z_far = 5  # Maximum depth in meters\n",
    "\n",
    "# Generate and visualize point cloud\n",
    "pcd = generate_point_cloud(\n",
    "    disparity_map=disparity,\n",
    "    intrinsic_file=intrinsic_file,\n",
    "    scale=scale,\n",
    "    z_far=z_far,\n",
    "    denoise_cloud=True,\n",
    "    denoise_nb_points=30,\n",
    "    denoise_radius=0.03,\n",
    "    output_dir=\"output/\"\n",
    ")\n",
    "\n",
    "# Visualize point cloud\n",
    "print(\"Visualizing point cloud. Press ESC to exit.\")\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "vis.add_geometry(pcd)\n",
    "vis.get_render_option().point_size = 1.0\n",
    "vis.get_render_option().background_color = np.array([0.5, 0.5, 0.5])\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
